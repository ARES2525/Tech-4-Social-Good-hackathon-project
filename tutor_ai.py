import os
import pickle
import streamlit as st
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_groq import ChatGroq

# =========================
# 1Ô∏è‚É£ Load API Key from .env or Environment
# =========================
load_dotenv()  # loads .env if available
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

if not GROQ_API_KEY:
    st.error("‚ùå Missing GROQ_API_KEY. Please set it in your environment or .env file.")
    st.stop()

# =========================
# 2Ô∏è‚É£ Streamlit Setup
# =========================
st.set_page_config(page_title="Climate AI Tutor üå±", page_icon="üåç", layout="wide")
st.title("üåç Climate AI Tutor")
st.subheader("Ask me anything about climate change, sustainability, and eco-friendly actions!")

# Sidebar settings
st.sidebar.header("‚öôÔ∏è Settings")
temperature = st.sidebar.slider("Creativity (temperature)", 0.0, 1.0, 0.3, 0.1)
age_group = st.sidebar.selectbox("Target Age Group", ["child", "teen", "adult"])
language = st.sidebar.selectbox("Preferred Language", ["English", "Hindi", "Spanish"])

# =========================
# 3Ô∏è‚É£ Load or Initialize FAISS (Cached)
# =========================
@st.cache_resource
def init_retriever():
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    if os.path.exists("climate_faiss.pkl"):
        with open("climate_faiss.pkl", "rb") as f:
            vectorstore = pickle.load(f)
    else:
        urls = [
            "https://climate.nasa.gov/evidence/",
            "https://www.ipcc.ch/report/ar6/syr/",
            "https://www.unep.org/resources/emissions-gap-report-2023"
        ]
        docs = []
        for url in urls:
            loader = WebBaseLoader(url)
            docs.extend(loader.load())
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
        split_docs = text_splitter.split_documents(docs)
        vectorstore = FAISS.from_documents(split_docs, embeddings)
        with open("climate_faiss.pkl", "wb") as f:
            pickle.dump(vectorstore, f)
    return vectorstore.as_retriever(search_kwargs={"k": 3})

retriever = init_retriever()

# =========================
# 4Ô∏è‚É£ Initialize Groq LLM (Cached)
# =========================
@st.cache_resource
def init_llm():
    return ChatGroq(
        api_key=GROQ_API_KEY,
        model="llama-3.3-70b-versatile",  # ‚úÖ latest 70B model
        temperature=temperature,
        max_tokens=512
    )

llm = init_llm()

# =========================
# 5Ô∏è‚É£ Prompt Template
# =========================
template = """
You are a Climate Tutor AI.

- Adapt your answer to the age group: {age_group}.
- Use simple terms for children, relatable analogies for teens, and scientific accuracy for adults.
- Answer in the user's preferred language: {language}.
- Use the retrieved knowledge below to ensure accuracy:
{context}

User Question: {question}
"""

prompt = PromptTemplate(
    input_variables=["age_group", "language", "context", "question"],
    template=template
)

# =========================
# 6Ô∏è‚É£ User Input & Response
# =========================
user_prompt = st.text_area("‚úçÔ∏è Enter your climate question or topic:", placeholder="e.g., How does deforestation affect global warming?")

if st.button("üîé Get Answer"):
    if not user_prompt.strip():
        st.warning("Please enter a question first.")
    else:
        with st.spinner("Fetching info and generating AI response (may take time for 70B model)..."):
            try:
                # Retrieve relevant docs
                docs = retriever.invoke(user_prompt)
                context = "\n\n".join([d.page_content for d in docs])

                # Format prompt
                final_prompt = prompt.format(
                    age_group=age_group,
                    language=language,
                    context=context,
                    question=user_prompt
                )

                # Get AI response
                response = llm.invoke(final_prompt)
                st.success("‚úÖ AI Response:")
                st.write(response.content)
            except Exception as e:
                st.error(f"Error: {e}")

# =========================
# 7Ô∏è‚É£ Suggested Questions
# =========================
st.markdown("---")
st.markdown("üí° **Try asking:**")
examples = [
    "What are the top 5 renewable energy sources?",
    "Explain carbon footprint in simple terms for kids.",
    "How can schools reduce their energy consumption?",
    "What are the effects of plastic pollution on oceans?",
]
for ex in examples:
    st.code(ex, language="text")

st.markdown("üå± *Powered by Groq AI*")
